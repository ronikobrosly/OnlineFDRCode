"""
GAI++ with memory
"""

from os.path import expanduser
import pdb

import matplotlib.pyplot as plt
import numpy as np
from numpy import sqrt, log, exp, mean, cumsum, sum, zeros, ones, argsort, argmin, argmax, array, maximum
import pandas as pd


# Desired false discovery rate
alpha0 = 0.01

# How many hypotheses will you test? (This should equal the length of the `pvec` array)
numhyp = 1000

# Related to penalty weight (default = 0.1)
startfac = 0.1

# Memory parameter
mempar = 1

# Prior weight vector
prw_vec = np.array(
    [
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.
    ]
)

# Penalty weight vector
penw_vec = np.array(
    [
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.
    ]
)

# Discount factor
tmp = range(1, 10000)
gamma_vec =  np.true_divide(log(np.maximum(tmp, ones(len(tmp))*2)), np.multiply(tmp, exp(sqrt(log(np.maximum(ones(len(tmp)), tmp))))))
gamma_vec = gamma_vec / float(sum(gamma_vec))  


# Vector of p-values (this would be streamed in real life, and not known ahead of time)
pvec = [0.3662121713019626, 0.791890530342962, 0.5149097407147354, 1.9099025077960618e-12, 0.34809137647588595, 0.5714255452896568, 0.07681558205128416, 4.7398254469960366e-07, 0.2738210523317355, 0.01796478244496962, 0.7705417933957502, 0.8340524235176036, 0.13352640570105423, 0.714412232763125, 0.3011515650247023, 0.7309385712459053, 0.3227301578962867, 0.39305205386659237, 0.13382095408737754, 0.2513706538331516, 0.5721846701766449, 6.9720185563531216e-06, 0.5529655169198984, 0.02816613988675697, 0.10759288337099719, 0.8320221469859787, 0.9228066793698488, 0.5691816425619368, 0.5460056538799298, 0.02326152572780262, 0.031059311061427126, 0.24331207578988878, 0.1148257571868451, 0.9183395936781938, 0.5570222646268173, 0.427042037942407, 0.7590314999914021, 7.822370137570923e-06, 0.8474115165532642, 0.9837213178435971, 0.4941955434836067, 0.4183095709040541, 0.9274759944239316, 0.07489541516493595, 0.981710752317497, 0.8898613415144759, 0.49317249128910823, 0.42461099824935244, 0.4526001971293112, 0.28352655383768366, 0.5757644976267571, 0.08710225780275309, 0.14330715193696983, 0.9999038355747183, 0.8020745907536104, 0.4348318024325717, 0.8087593174469173, 0.7428699520780486, 0.1469340728222841, 0.7208033450519641, 0.24748214308034133, 0.46726525057089574, 0.8640022443471742, 0.3820267620900657, 0.5676296585544929, 2.0363828720117475e-10, 0.375432718537574, 0.38529774075501555, 0.46046842801524823, 0.8369533180256767, 0.5889230762713866, 0.9593434405196891, 0.7433211421399275, 0.42455203118409723, 0.30049569025048495, 0.4754554350900797, 0.23141883001722763, 0.05376071673337567, 0.6070544591269593, 0.04446592988857527, 0.2866080662164203, 0.24599901723094963, 0.16725441466984825, 0.09310256073471217, 0.6782738095636238, 0.9093386170543295, 0.23887839554145096, 0.9299604410473669, 0.9611430578839496, 0.5869981452190044, 0.24307254828544644, 4.528146150926349e-05, 0.7103414809733037, 0.16191961246837394, 0.005789485426898343, 0.17760316038400914, 0.4438618997629833, 0.8065092884620109, 0.8104367025965483, 0.8737288891110154, 0.4398519325078887, 0.8083127837625104, 0.5152162962791796, 0.2212346713897192, 0.651126183830703, 0.9789082841531508, 0.6430276348820629, 0.6491044895033256, 0.4570421000152751, 0.026828732996027414, 0.5352560969593411, 0.797293723830705, 0.6462499310294835, 0.3177769608494013, 0.28126576735060005, 0.02386337617540974, 2.669222178763805e-11, 0.26620350421860817, 0.9314321809207847, 0.6351795801163249, 2.9567946367983936e-06, 0.9874372737052008, 0.6372068679406264, 0.41618319794148995, 0.5965744908802888, 0.39575778661230354, 0.5506382457453223, 0.39795510674826573, 0.46176800822147046, 0.10427793032272632, 0.39139468093916663, 0.09800299903604695, 0.35776952387747685, 0.17202912890851862, 0.7689609002922254, 0.9756309443500929, 0.869531760483155, 0.9376932977185914, 0.1883759354541653, 0.6200160929956984, 0.15791601163405977, 0.8244715277125493, 0.5549929818384816, 0.9874351336413063, 0.008786764548640507, 9.394405891280812e-15, 3.151824471242858e-07, 0.5510069084755665, 5.479324320074113e-07, 0.31618209333084246, 0.5864124896139826, 0.47419787822755033, 0.7733123456717574, 0.58158483259322, 0.7097681262495311, 0.23535168935870565, 0.7998467393735416, 0.7701036004161608, 0.14654191289659466, 0.8232954186773076, 0.12147118515551451, 0.7169011681728437, 0.9550562066977196, 0.28500194841136495, 0.40638327884734593, 0.3397045974792915, 0.9458790126485516, 0.2899825021313379, 3.424552298239464e-11, 1.683125891542681e-07, 0.30686031573073447, 0.6955668953254104, 0.12185183327102979, 0.12912606210330327, 0.49524754174655083, 0.3128047922450993, 0.1774647249555169, 0.0008387024098558928, 0.946542936698475, 0.8863388009514254, 0.027733388920771398, 0.23895002525614006, 0.9146087557305268, 0.5974539415216371, 0.1803439853711435, 0.6338565629530362, 0.7138955985735842, 0.99585781901192, 0.11356083794456696, 0.13394275620168877, 0.42981881027570146, 0.8264723870145563, 0.16834423973625734, 0.23470479107733355, 0.8954236774836056, 0.5122727874124233, 0.8637152959197103, 0.9168693557042777, 0.4979136410925944, 0.006373517122260906, 0.5011285275277101, 0.4105316287782451, 0.912169660943229, 0.22550628042001442, 0.2756312146796078, 0.8769456587314999, 0.30354144032777397, 0.9518931805512936, 0.5438441719114634, 0.1720789510633507, 0.4114135025950686, 0.07185951687981412, 0.09769433056640431, 0.9936327547257684, 0.2608022694061075, 0.30639848752019516, 0.28321373916606707, 0.5774027325225819, 0.2593433611331405, 0.4968492946002834, 0.37005891784704315, 0.003016050924356273, 0.031498297778687935, 0.8526025024240382, 0.17511568996212423, 0.753936906104114, 0.12266710244351565, 0.12320487201047946, 0.48429802051000637, 0.9639092772936537, 0.8859623979595198, 0.5260887120074511, 0.5909912430417834, 0.5312154515525868, 0.07455319148370013, 0.4096366851750213, 0.9353643881254936, 0.959232970949873, 0.08283816417943013, 0.06828461100291067, 0.7852096240007078, 0.7011324912920338, 0.5891315228155412, 0.539674910401257, 0.165661127578544, 0.012157196899486878, 0.608376690800406, 0.24311876103638863, 0.001059827159091723, 0.6369924966916982, 0.07400000056578986, 0.005249399760194876, 0.4589221441253599, 0.18229737736068774, 0.20000725000058672, 0.46039267756453317, 0.5486418930952357, 0.3539742822429084, 0.7139157785090736, 6.2070734171436854e-06, 0.7326291533671949, 0.6338763041366862, 0.9626999577327016, 0.3915843653210176, 0.07031818802889575, 0.8249238868720018, 0.06992668714243645, 0.6942008502922011, 0.2296802380323727, 0.07048177957472271, 0.9649524659273124, 0.8382700077546015, 0.8270676670610572, 0.8637174199938069, 0.32084320671015265, 0.868098225851796, 0.6569913541027053, 0.8715637803295218, 0.6997652907168104, 0.3965897485956148, 0.8216612986968201, 0.2103736802904207, 0.15434422169717557, 0.41385759219962237, 0.5609673829369553, 0.28524944732893753, 0.25544582667519655, 0.397703054386523, 0.841504107283483, 0.3920551019299371, 0.9622472012433867, 0.7051403555683708, 0.00020883357163229988, 0.5314482918207847, 0.1380423816361787, 0.4684954830745267, 0.9222803490359214, 0.8534700556038755, 0.000005, 0.0003963631239885352, 0.00001299, 0.0322, 0.0000099, 0.004, 0.000032, 0.37471223578017143, 0.5419287369737047, 0.13379633479267733, 0.8036284160443377, 8.855368599321688e-07, 0.9594309798060732, 0.0886290703465211, 0.8200912408860319, 1.9494794032822887e-10, 0.7339689014187434, 0.48328768268832356, 0.04598183372913228, 0.017774636487449907, 0.8020221676898178, 0.3612934885924932, 0.830156963913337, 0.9090801510479317, 0.8070934401606942, 0.5847356384322748, 0.7306579691833082, 0.4999291998936888, 0.5505350489327148, 0.9204636505944884, 0.1271800550433564, 0.4073816080219287, 0.4292474807208452, 0.1128490085160524, 0.5987345373517512, 0.33825134542774593, 0.9406448161519719, 0.1786972494942516, 0.6763802241845819, 0.12949593626233113, 0.3336666164928732, 0.8001073151920177, 0.2833296705418651, 0.7193053762900993, 0.07699520560794125, 0.6346397787451108, 0.04957167498563329, 0.792205869634438, 0.14847108198793285, 0.9864320984498431, 0.08238793487643467, 0.7990535936523937, 0.5989745487199023, 0.7375109349902474, 0.8929556033129854, 0.09572059020267447, 0.5088951444242946, 0.9820168911088554, 0.7757142642600086, 0.11651510486224508, 1.2229101146535423e-09, 0.15618960970522128, 0.9382987894968402, 0.29391299223497336, 0.642882820739217, 0.6190218472624736, 0.5984147242060078, 0.44053117412237786, 0.43968579201816826, 0.9346946356310845, 3.118111850142791e-06, 0.9234687995957402, 0.8140951316582106, 0.7863274925894581, 0.43745888954784806, 3.732933198666195e-10, 0.9876535670903797, 0.032126177936638425, 0.9769748275908262, 0.6133348605728298, 0.01730105596799591, 7.288667642370609e-09, 0.47241377522107, 0.5744390674872638, 0.981225459142337, 0.03582996992782521, 0.4860818309452851, 0.813750603556416, 0.8310579636977968, 0.005497803441483473, 0.14213850965568062, 0.5006537761694636, 0.6396433812506199, 0.6565633577111465, 0.705838226087048, 0.23907641456394324, 0.002398537139216651, 0.42008583197266003, 0.6538653406811692, 0.7173390460201499, 0.9550592889347445, 0.5288855026239735, 0.3309452580384623, 9.545190341812987e-08, 0.431635878242121, 0.8371823059166874, 0.3051465105421127, 0.585515639816677, 0.3538608443865986, 2.1526213467488806e-05, 0.8345932839005079, 0.6747425162480452, 0.9099699111287034, 0.4059583463112957, 0.00012527940308883274, 0.31728081458238355, 0.7213589347370049, 0.9819159367654015, 0.45941153692575365, 0.0426301170688579, 0.5054943670697536, 0.9458388783279277, 0.2225555454789493, 0.5135190147452184, 0.9289558511139511, 0.13193004858720705, 0.6099758659479264, 0.6063719305263424, 0.05993863606879061, 0.8714371955846053, 0.851759771928991, 0.7074431137854225, 0.10794471603839818, 0.6067342008326707, 0.5937775570585644, 0.7368149555993518, 0.9194107219556998, 0.19645852841162137, 0.07641902206745295, 0.7145360263587206, 0.06856824068926745, 0.1114777290210137, 0.03129926482126397, 0.0013610082398797527, 0.792695854489678, 0.9843387046902122, 0.27496065208240494, 0.6771042571497715, 0.5324446790905778, 0.47937859486560663, 0.028547613374684682, 0.6939978419955619, 0.2245610185259972, 0.591446245568932, 0.8650331204676848, 0.2845382947097638, 0.5152224333852953, 0.2408721417085702, 0.37001652215742165, 0.8232462137691502, 0.6914836489081193, 0.9448617996726594, 0.003272901292412062, 0.9415548348182624, 0.5238425963638222, 0.7058633825316549, 0.7481136480854684, 0.45558187918201465, 0.7570638109302732, 0.5546109394936162, 0.6349397680315114, 0.27969642148722695, 0.5503810383084882, 0.618278235184724, 0.5672555086060052, 0.2070634266893756, 0.09381537422499711, 0.5406418238349208, 0.461004556746492, 0.42034664430590085, 0.3324480011228338, 0.002480611936272826, 0.1188281418994201, 0.30349953604515223, 0.2667535666697868, 0.02812314329787461, 0.10364593045811606, 0.5707049633061727, 0.47278869422687286, 0.3248677897669565, 0.11402363887254478, 0.13315681853542444, 0.42011110577638766, 0.8837335353826706, 0.26548188289068897, 0.684417275331848, 0.1256682168311549, 0.4234184364783301, 0.9662875017911576, 0.08664504808640569, 0.8773735963886947, 0.6776135076313355, 0.7573724522294705, 0.07305171991000578, 0.10206915974173907, 0.1618774621737854, 0.925846611923162, 0.0696040355425289, 0.8320280059278131, 0.04832450512153691, 0.2589012774087166, 0.13699439468969077, 0.032089675106324975, 2.067291121616567e-13, 0.4028203521672884, 0.7694574700851479, 0.06889030140233236, 0.569258555428841, 0.41339085502058914, 0.9728793334934017, 0.49511898887112793, 0.15389970870902978, 0.3687989464445789, 0.6060280000818914, 0.46234811271112486, 0.2349937529530387, 0.9755356701217363, 0.12397862272040264, 0.14849744914975643, 0.3307643854868213, 0.3488765515925616, 0.17238771891386018, 0.3728752444808773, 0.07353198977438713, 0.997062452865951, 0.10643818926517858, 0.45383938833379, 0.04210564351492346, 0.4764815850995904, 0.20487207076384173, 0.33932184388792364, 0.40046740282201176, 0.026550099206070227, 0.7696622682894769, 0.328444561759921, 1.9859482851608944e-06, 0.1703943703585079, 0.1811738040270765, 0.3496417202590142, 0.2673805747867738, 0.36104749814352644, 0.8464341257055711, 0.9112105526035005, 0.40184260145625006, 0.7015865379900773, 0.4329062721030822, 0.5450113657757092, 0.3357725636191351, 0.17882716820157296, 0.23214261381529588, 0.3429967368236654, 0.4900378509596961, 0.5517454222432695, 0.8960067432009747, 0.8306671641078591, 0.10033559190016902, 0.5436362058949031, 0.6776823943297408, 0.3354779641485377, 0.8777510038236785, 0.29621869076153373, 0.13760112402679792, 0.5508021129331524, 0.36342797846218255, 0.5160870029558653, 0.4658603118612197, 0.5500911524471774, 0.8795957835153967, 0.00897031095032032, 0.8105659280404287, 0.19222140759363515, 0.14005953265043505, 0.9566710794546964, 0.5247355789513004, 0.014317373303094261, 0.6346791086946282, 0.3624240435743682, 0.5175997130734713, 0.5928488244410565, 0.49087483175275237, 0.7167477644597449, 0.5617055990063335, 0.7448929420598434, 0.3747476903211471, 0.058888904932272744, 0.9331514563859737, 0.010063289351535975, 0.5227987449999075, 0.9508831958541754, 0.1497495984890106, 0.4709064492102044, 0.763121123329042, 0.9391866656935294, 0.39872791543867836, 0.7205610803310052, 0.9319478995114616, 0.1758261602083223, 0.3943583808043747, 0.5373701452720067, 0.22030739213535488, 0.23148977188943154, 0.5304544515606462, 0.30109795994979127, 0.9121932816969706, 0.006422450877672777, 0.1676029312506353, 0.7553923732615996, 0.04676926509678496, 0.2022036301926552, 0.2743186195266497, 0.33874585325942197, 0.8693767750279874, 0.35395046051397516, 3.7590844177010157e-06, 0.9177642181726232, 0.10096238747464334, 0.5857382759092935, 0.8253816842282958, 0.6068005337102728, 0.9347043845432291, 0.7331535428608039, 0.032141548252975216, 0.27050893277263044, 0.34259093995256473, 0.892866375307144, 0.5054426628438532, 0.6848471174505838, 0.686506462523911, 0.9688770820414518, 0.7895098613971117, 0.5079139878376181, 0.1606454724761024, 0.07268771566043958, 0.06022611814243029, 0.4115001544557183, 0.9955698386697714, 0.49804158530393117, 0.9781212073217124, 0.9607781950757153, 0.35632240593593645, 0.4619919366297939, 0.3358353744992314, 0.6397247339634264, 0.5831963263852894, 0.2195665129597698, 0.40571937122447943, 0.7514485773949608, 0.5916696375232968, 0.17570950067996205, 0.9168511794699504, 0.8402017021853836, 0.7875426825393543, 0.24216162907508387, 0.836611086445144, 0.7609735827197573, 0.6717159587252014, 0.9708802602433251, 0.758417585781519, 0.6499079998336459, 3.144688972784867e-09, 0.5843782233421583, 0.5538282420709425, 0.3757582839480912, 0.9523407839397667, 0.7066604467509482, 0.3132312679148892, 0.7604905765497227, 0.7753047955468692, 0.6514425564579642, 5.141693227376053e-10, 0.056046824579468954, 0.7631144281820298, 0.3626476815171342, 0.1742492418525896, 0.3122905349579034, 0.9866133431622904, 0.4338389365267208, 0.8275091720656882, 0.08600847143586451, 0.022312995406920822, 0.08860847355524273, 0.03254716887243636, 0.11458597921677907, 0.017581405551603996, 0.003859568897882412, 0.16211485617909638, 0.8337646358121963, 0.12922231993263733, 0.9240780684912424, 0.01769010339720259, 0.14811190910653085, 0.9614446670563578, 0.8989666039937333, 0.27387798554488196, 0.19871865564274394, 0.11139489546728441, 0.09081818096074457, 0.5912430745316951, 0.277567743639229, 0.6101265422703388, 0.23861961774449414, 0.8842608997628936, 0.3320116241060458, 0.6356163813008628, 0.6969322992738409, 0.7112308643189877, 5.831813813619729e-13, 0.47266588573057766, 0.18691382072378426, 0.899256720428018, 0.43158402453388, 0.8991797613629882, 0.7233897625589367, 0.2801470575652363, 0.12809252999936013, 0.7410152862532671, 0.5546845397944054, 0.38946745794977156, 0.7654568693160795, 0.7492421627088088, 0.622797468248224, 0.8419299140642537, 0.30922282529590417, 0.21475344331845836, 0.0013787357001667211, 0.7640398624695932, 0.9947653716270085, 0.8593055703974243, 0.13557631969220876, 0.28571372657993044, 0.014733102739436642, 0.5994842686786799, 0.39298325684419566, 0.09617638313208828, 0.7732210213867388, 0.7791612446381608, 0.05179257174527534, 0.339670678792134, 0.11465982900751531, 0.47129220837570795, 0.32699200377196735, 0.2255317363339483, 9.855385952081662e-12, 0.6063445183205812, 0.620502903473832, 0.47165089132133997, 0.7132422182170354, 0.08557629336408784, 0.10475194296296941, 0.8158840736463089, 0.10122044939241974, 0.9392062986102956, 0.6724980346903902, 0.7701622154625615, 0.06840382281875686, 0.19996813023241, 0.005314248138753822, 0.911874715968851, 0.35832991122978775, 0.9057595601144658, 0.826580370634729, 0.34165716643543076, 0.6386665269335232, 0.23997585842721825, 0.4730098809696103, 0.6885204436090167, 0.6164238089196646, 1.4668983382544142e-12, 0.8406061148268913, 0.5016489955898822, 0.7719665608369197, 0.6352184616653579, 0.8724023670846475, 0.8681178435328156, 0.13812176581649366, 0.4520714908397232, 0.5101167665938258, 0.9529731182499482, 0.506832169035105, 0.1913945173415904, 0.7709977122295273, 0.7703836262697411, 0.7198330608833945, 0.9863061243603422, 0.6749172351557764, 0.21815620684504744, 0.8226990348127957, 0.3446939270793803, 0.7651201083291582, 0.04714041814973484, 0.6556366469007273, 6.077050870932388e-09, 0.12893650971495285, 0.9961429074837265, 0.6228661053439679, 0.23463443429335318, 0.1799162393128414, 4.1058607535354063e-10, 0.6325807475343936, 0.5259402039570397, 0.7984555118346062, 0.11994648545501269, 0.663197391816981, 0.10813276562421661, 0.08571516259785018, 0.9948620693877736, 0.7571774938836422, 0.33025854849179437, 0.8388148916850564, 0.5137237332484526, 0.007939549224041711, 0.05395309440081678, 0.03991250633011464, 0.1695576364057213, 0.5337844434372114, 0.38967735995055086, 0.1538549825728828, 0.399712933951864, 0.37509767907595737, 0.010400623266334041, 0.8315813809177511, 0.3886335911610771, 0.028972594114146502, 0.6160517567898889, 0.48408214329438526, 0.09035049165637059, 0.21578130324706046, 0.33171765346040893, 0.5144136737392155, 0.58771829702776, 0.40322908706769145, 0.36966943348497594, 0.5332831758260673, 0.9056308209078423, 0.39957275389203273, 0.2728923764338168, 0.1535130449177182, 0.19356853280810737, 0.9336537565057299, 0.30074237485514066, 0.31032995718871303, 0.021147674734203446, 0.10446015243795717, 0.5263900117972728, 0.505825397337867, 0.1326818953936797, 0.5684060418001544, 0.9333435811982708, 0.45013562562974085, 0.36971618389227934, 0.8733779730161959, 0.8143887702530374, 0.49121708392267416, 0.3454414045225884, 0.33851577044949366, 0.4384762402763971, 0.9810185814315886, 0.20338646507939262, 0.37901851424247945, 0.1923067456368679, 0.6355667981540261, 0.00040370920956185776, 0.00021291676043938112, 0.6905665123993396, 0.005485881633085358, 0.25235329556005504, 0.024287215120293637, 0.038487945862063, 0.20146775938426786, 0.6990378527296781, 0.2662054204126296, 0.6739328227606527, 0.008766321029843758, 0.7911121094803737, 0.08722844696798616, 0.05135014085121044, 0.17730938568622256, 0.29108344249281004, 0.001973878182112775, 0.8982842518573807, 0.10529508890221805, 0.6679882485534661, 0.6058816379734608, 0.7707579415011809, 0.9530698471630209, 0.3070003073141805, 0.9258699127045396, 0.5726779580438939, 0.25404343272852725, 0.32998909452250935, 0.6663413417937514, 0.5325993712633313, 0.4466108835239626, 0.8646619941760746, 0.209829015053156, 0.895822644281034, 0.6779013152630604, 0.34786629388215407, 0.38240655020332825, 0.21924596186893808, 0.016011434457018085, 0.23372342808185453, 0.03254893979429, 0.32853070886005253, 0.7113108720581168, 0.35314872507763795, 0.7134383441810574, 0.5877610597441469, 0.2581299098829408, 0.779004244820557, 0.28729672944598816, 0.8633043025970005, 0.6054952643236214, 0.7810542048783707, 0.37447467235245924, 0.6744143391009303, 0.8403404372283613, 0.8454147306093747, 0.021197946234194243, 0.6924706457374377, 0.0007966219486343148, 0.8951559925550255, 0.14183605806285937, 0.8379083156515298, 0.10235943641165295, 0.07714070778222537, 0.3428826888890535, 0.7303160470326966, 0.5972960355460333, 0.4539219782001356, 0.5612019970537094, 0.4567847450432635, 0.9496653851434949, 0.9495933742282024, 0.49964857600061485, 0.928881671553148, 0.5660616654018804, 0.2935891761405619, 0.4801802063042466, 0.9112384460736994, 0.9944995259471832, 0.40966339468211566, 0.8570399209350701, 0.2793581246720148, 0.0375008028833876, 0.6869757631478616, 0.04291995262981863, 0.19273841913375833, 0.5504153001733747, 0.46891355633686027, 0.4449654108990777, 0.07584873045563545, 0.13909171664975248, 0.8192868319217973, 0.27656336129222336, 0.6751736324707719, 0.27749402468937745, 0.40773468231464716, 0.14166845975672082, 0.44271894235548803, 0.4639709197559042, 0.4379632558973334, 0.0009708964698895971, 0.44393051162320596, 0.877616316992996, 0.2699658158268071, 0.37564763011474755, 0.8429426649162981, 0.14946403681570405, 0.427967915353601, 0.3001389854338815, 0.011000050547902112, 0.697868756864924, 0.7321102145814291, 0.23362989172947835, 0.17552974068979388, 0.933642579059839, 0.48130216870145826, 0.8864462369861238, 0.358661870993094, 0.36710740418566234, 0.08501144001750024, 2.523878102881025e-16, 0.8589112819743552, 0.3266767558418723, 0.22396458195579627, 0.10852404312321563, 0.06503467099603864, 0.8019190363744558, 0.5974259188140678, 0.4508036644498117, 0.8677180316876882]





# If you want you can add different thresholds here, but for now not doing anything
def thresh_func(x):
    return x


def run_fdr(pvec, numhyp, gamma_vec, alpha0, startfac, mempar, prw_vec, penw_vec):

    pen_min = min(penw_vec)
    w0 = min(pen_min, startfac)*alpha0
    wealth_vec = np.zeros(numhyp + 1)
    wealth_vec[0] = w0
    alpha = np.zeros(numhyp + 1)
    alpha[0:2] = [0, gamma_vec[0]*w0]  # vector of alpha_js
    last_rej = 0 # save last time of rejection

    numhyp = len(pvec)
    last_rej = 0
    first = 0 # Whether we are past the first rejection
    flag = 1
    rej = np.zeros(numhyp+1)
    phi = np.zeros(numhyp+1)
    psi = np.zeros(numhyp+1)

    # Have to change that since prior weights need to be adjusted 
    r = [np.true_divide(thresh_func(penw_vec[i]), prw_vec[i]) for i in range(numhyp)]

    #penw, prw, pvec without k+1 (just shifted for purpose of having a wealth[0] (rest starts at 1)
    # here t=k+1 (t in paper)
    last_rej = []
    psi_rej = []

    for k in range(0, numhyp):                

        if wealth_vec[k] > 0:
            this_alpha = alpha[k+1] # make sure first one doesn't do bullshit

            # Calc b, phi
            b_k = alpha0 - (1-first)*np.true_divide(w0, penw_vec[k])
            phi[k + 1] = min(this_alpha, mempar*wealth_vec[k] + (1-mempar)*flag*w0)
            # Adjust prior weight depending on penw, phi, b - calc prw, r
            max_weight = (phi[k+1]*thresh_func(penw_vec[k]))/((1-b_k)*this_alpha)
            prw_vec[k] = min(prw_vec[k], max_weight)
            r[k] = np.true_divide(thresh_func(penw_vec[k]), prw_vec[k])

            # Calc psi
            # The max is to get rid of numerical issues when computing max_weight
            psi[k + 1] = max(min(phi[k + 1] + penw_vec[k]*b_k, np.true_divide(phi[k + 1], this_alpha)*r[k] - penw_vec[k] + penw_vec[k]*b_k),0)

        
            # Rejection decision
            rej[k + 1] = (pvec[k] < np.true_divide(this_alpha,r[k]))

            if (rej[k + 1] == 1):
                if (first == 0):
                    first = 1
                last_rej = np.append(last_rej, k + 1).astype(int)
                psi_rej = np.append(psi_rej, psi[k + 1]).astype(float)
            
            # Update wealth
            wealth = mempar*wealth_vec[k] + (1-mempar)*flag*w0 - phi[k + 1] + rej[k + 1]*psi[k + 1]

            # Calc new alpha
            
            if len(last_rej) > 0:
                # first_gam = mempar**(k+1-last_rej[0])*gamma_vec[k + 1 - last_rej[0]]
                #t_taoj = ((k+1)*np.ones(len(last_rej[0:-1]),dtype=int) - last_rej[0:-1])
                # sum_gam = sum(np.multiply(mempar**t_taoj, gamma_vec[t_taoj])) - first_gam + gamma_vec[k+1 - last_rej[-1]]
                # next_alpha = gamma_vec[k+1]*wealth_vec[0] + (alpha0 - w0/float(penw_vec[last_rej[0]]))*first_gam + alpha0*sum_gam

                #gam_vec = np.append(np.multiply(mempar**t_taoj, gamma_vec[t_taoj]), gamma_vec[k + 1 - last_rej[-1]])

                t_taoj = ((k+1)*np.ones(len(last_rej),dtype=int) - last_rej)
                gam_vec = np.multiply(mempar**t_taoj, gamma_vec[t_taoj])

                next_alpha = gamma_vec[k + 1]*wealth_vec[0] + np.dot(psi_rej, gam_vec)

            else:
                sum_gam = 0
                next_alpha = gamma_vec[k+1]*wealth_vec[0]

            # next_alpha = mempar**(k+1-last_rej)*gamma_vec[k+1 - last_rej]*wealth_vec[last_rej]
            #next_alpha = gamma_vec[k+1 - last_rej]*wealth_vec[last_rej]
        else:
            break

        wealth_vec[k + 1] = wealth
        if k < numhyp-1:
            alpha[k+2] = next_alpha
        
        # After past the first reject, set flag to 0
        if (first == 1):
            flag = 0

    # Cut off the first zero
    rej = rej[1:]
    alpha = alpha[1:]

    return rej
    
    

rej = run_fdr(pvec, numhyp, gamma_vec, alpha0, startfac, mempar, prw_vec, penw_vec)


df = pd.DataFrame(
    {
        'time_step': [i for i in np.arange(numhyp)],
        'pvalue': pvec,
        '-log10_pvalue': [-np.log10(i) for i in pvec],
        'reject': rej
    }
)

df_reject = df[df["reject"] == 1]

plt.figure(figsize=(12,6))
ax = plt.subplot(111)
ax.spines["top"].set_visible(False)
ax.spines["right"].set_visible(False)
plt.xlabel("Time step", fontsize=14)
plt.ylabel("-log10(p-value)", fontsize=14)
plt.title(f"FDR = {alpha0}", fontsize=16)
plt.plot(df["time_step"], df["-log10_pvalue"], '-', linewidth=0.5, markersize=1.5)
plt.plot(df_reject["time_step"], df_reject["-log10_pvalue"], 'o', color = 'red', markersize=3)
plt.axhline(y=1.301, color='r', linewidth=0.8, linestyle='--', label = "p = 0.05")
plt.legend(loc="upper left")
plt.savefig(expanduser("~/Desktop/p-values.png"), dpi = 300)
